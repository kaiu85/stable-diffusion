{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les tokenizers rapides dans le pipeline de QA (TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.97773,\n",
       " 'start': 78,\n",
       " 'end': 105,\n",
       " 'answer': 'Jax, PyTorch and TensorFlow'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "ü§ó Transformers is backed by the three most popular deep learning libraries\n",
    " ‚Äî Jax, PyTorch, and TensorFlow ‚Äî with a seamless integration between them. \n",
    "It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "# ü§ó Transformers s'appuie sur les trois biblioth√®ques d'apprentissage profond les plus populaires\n",
    "# (Jax, PyTorch et TensorFlow) avec une int√©gration transparente entre elles.\n",
    "# C'est simple d'entra√Æner vos mod√®les avec l'une avant de les charger pour l'inf√©rence avec l'autre.\n",
    "question = \"Which deep learning libraries back ü§ó Transformers?\"\n",
    "# Quelles biblioth√®ques d'apprentissage profond derri√®re ü§ó Transformers ?\n",
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.97149,\n",
       " 'start': 1892,\n",
       " 'end': 1919,\n",
       " 'answer': 'Jax, PyTorch and TensorFlow'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_context = \"\"\"\n",
    "ü§ó Transformers: State of the Art NLP\n",
    "\n",
    "ü§ó Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
    "question answering, summarization, translation, text generation and more in over 100 languages.\n",
    "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
    "\n",
    "ü§ó Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
    "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
    "can be modified to enable quick research experiments.\n",
    "\n",
    "Why should I use transformers?\n",
    "\n",
    "1. Easy-to-use state-of-the-art models:\n",
    "  - High performance on NLU and NLG tasks.\n",
    "  - Low barrier to entry for educators and practitioners.\n",
    "  - Few user-facing abstractions with just three classes to learn.\n",
    "  - A unified API for using all our pretrained models.\n",
    "  - Lower compute costs, smaller carbon footprint:\n",
    "\n",
    "2. Researchers can share trained models instead of always retraining.\n",
    "  - Practitioners can reduce compute time and production costs.\n",
    "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
    "\n",
    "3. Choose the right framework for every part of a model's lifetime:\n",
    "  - Train state-of-the-art models in 3 lines of code.\n",
    "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
    "  - Seamlessly pick the right framework for training, evaluation and production.\n",
    "\n",
    "4. Easily customize a model or an example to your needs:\n",
    "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
    "  - Model internals are exposed as consistently as possible.\n",
    "  - Model files can be used independently of the library for quick experiments.\n",
    "\n",
    "ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch and TensorFlow ‚Äî with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "\n",
    "long_context - fr = \"\"\"\n",
    "ü§ó Transformers : l'√©tat de l'art du NLP\n",
    "\n",
    "ü§ó Transformers fournit des milliers de mod√®les pr√©-entra√Æn√©s pour effectuer des t√¢ches sur des textes telles que la classification, \n",
    "l'extraction d'informations, la r√©ponse √† des questions, le r√©sum√© de textes, la traduction, la g√©n√©ration de texte et plus encore dans plus de 100 langues.\n",
    "Son objectif est de rendre le traitement automatique des langues de pointe plus facile √† utiliser pour tout le monde.\n",
    "\n",
    "ü§ó Transformers fournit des API permettant de t√©l√©charger et d'utiliser rapidement ces mod√®les pr√©-entra√Æn√©s sur un texte donn√©, de les affiner sur vos propres ensembles de donn√©es et de les partager avec la communaut√© sur notre site Web.\n",
    "puis de les partager avec la communaut√© sur notre hub de mod√®les. En m√™me temps, chaque module python d√©finissant une architecture est enti√®rement autonome et peut √™tre modifi√© pour permettre des exp√©riences de recherche rapides.\n",
    "peut √™tre modifi√© pour permettre des exp√©riences de recherche rapides.\n",
    "\n",
    "Pourquoi devrais-je utiliser des transformateurs ?\n",
    "\n",
    "1. Des mod√®les de pointe faciles √† utiliser :\n",
    "  - Haute performance sur les t√¢ches NLU et NLG.\n",
    "  - Faible barri√®re √† l'entr√©e pour les √©ducateurs et les praticiens.\n",
    "  - Peu d'abstractions pour l'utilisateur avec seulement trois classes √† apprendre.\n",
    "  - Une API unifi√©e pour utiliser tous nos mod√®les pr√©-entra√Æn√©s.\n",
    "  - Des co√ªts de calcul plus faibles, une empreinte carbone r√©duite :\n",
    "\n",
    "2. Les chercheurs peuvent partager les mod√®les form√©s au lieu de toujours les reformer.\n",
    "  - Les praticiens peuvent r√©duire le temps de calcul et les co√ªts de production.\n",
    "  - Des dizaines d'architectures avec plus de 10 000 mod√®les pr√©-form√©s, certains dans plus de 100 langues.\n",
    "\n",
    "3. Choisissez le cadre appropri√© pour chaque √©tape de la vie d'un mod√®le :\n",
    "  - Entra√Ænez des mod√®les de pointe en 3 lignes de code.\n",
    "  - D√©placez un seul mod√®le entre les frameworks TF2.0/PyTorch √† volont√©.\n",
    "  - Choisissez de mani√®re transparente le bon framework pour l'entra√Ænement, l'√©valuation et la production.\n",
    "\n",
    "4. Adaptez facilement un mod√®le ou un exemple √† vos besoins :\n",
    "  - Nous fournissons des exemples pour chaque architecture afin de reproduire les r√©sultats publi√©s par ses auteurs originaux.\n",
    "  - Les √©l√©ments internes des mod√®les sont expos√©s de mani√®re aussi coh√©rente que possible.\n",
    "  - Les fichiers de mod√®les peuvent √™tre utilis√©s ind√©pendamment de la biblioth√®que pour des exp√©riences rapides.\n",
    "\n",
    "ü§ó Transformers s'appuie sur les trois biblioth√®ques d'apprentissage profond les plus populaires (Jax, PyTorch et TensorFlow) avec une int√©gration parfaite\n",
    "entre elles. Il est simple d'entra√Æner vos mod√®les avec l'une avant de les charger pour l'inf√©rence avec l'autre.\n",
    "\"\"\"\n",
    "question_answerer(question=question, context=long_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"tf\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 66) (1, 66)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "# Masque tout, sauf les tokens du contexte\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "# D√©masquer le token [CLS]\n",
    "mask[0] = False\n",
    "mask = tf.constant(mask)[None]\n",
    "\n",
    "start_logits = tf.where(mask, -10000, start_logits)\n",
    "end_logits = tf.where(mask, -10000, end_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_probabilities = tf.math.softmax(start_logits, axis=-1)[0].numpy()\n",
    "end_probabilities = tf.math.softmax(end_logits, axis=-1)[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = start_probabilities[:, None] * end_probabilities[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.triu(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97773"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_index = scores.argmax().item()\n",
    "start_index = max_index // scores.shape[1]\n",
    "end_index = max_index % scores.shape[1]\n",
    "print(scores[start_index, end_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "start_char, _ = offsets[start_index]\n",
    "_, end_char = offsets[end_index]\n",
    "answer = context[start_char:end_char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Jax, PyTorch and TensorFlow',\n",
       " 'start': 78,\n",
       " 'end': 105,\n",
       " 'score': 0.97773}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {\n",
    "    \"answer\": answer,\n",
    "    \"start\": start_char,\n",
    "    \"end\": end_char,\n",
    "    \"score\": scores[start_index, end_index],\n",
    "}\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(question, long_context)\n",
    "print(len(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\"\"\n",
       "[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP\n",
       "\n",
       "[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
       "question answering, summarization, translation, text generation and more in over 100 languages.\n",
       "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
       "\n",
       "[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
       "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
       "can be modified to enable quick research experiments.\n",
       "\n",
       "Why should I use transformers?\n",
       "\n",
       "1. Easy-to-use state-of-the-art models:\n",
       "  - High performance on NLU and NLG tasks.\n",
       "  - Low barrier to entry for educators and practitioners.\n",
       "  - Few user-facing abstractions with just three classes to learn.\n",
       "  - A unified API for using all our pretrained models.\n",
       "  - Lower compute costs, smaller carbon footprint:\n",
       "\n",
       "2. Researchers can share trained models instead of always retraining.\n",
       "  - Practitioners can reduce compute time and production costs.\n",
       "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
       "\n",
       "3. Choose the right framework for every part of a model's lifetime:\n",
       "  - Train state-of-the-art models in 3 lines of code.\n",
       "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
       "  - Seamlessly pick the right framework for training, evaluation and production.\n",
       "\n",
       "4. Easily customize a model or an example to your needs:\n",
       "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
       "  - Model internal [SEP]\n",
       "\"\"\"\n",
       "\n",
       "\"\"\"\n",
       "[CLS] Quelles sont les biblioth√®ques d'apprentissage profond qui soutiennent [UNK] Transformers ? [SEP] [UNK] Transformers : l'√©tat de l'art du NLP\n",
       "\n",
       "[UNK] Transformers fournit des milliers de mod√®les pr√©-entra√Æn√©s pour effectuer des t√¢ches sur des textes telles que la classification, l'extraction d'informations, la r√©ponse √† des questions, le r√©sum√©, la traduction, la g√©n√©ration de textes, etc,\n",
       "la r√©ponse √† des questions, le r√©sum√©, la traduction, la g√©n√©ration de texte et plus encore dans plus de 100 langues.\n",
       "Son objectif est de rendre le traitement automatique des langues de pointe plus facile √† utiliser pour tous.\n",
       "\n",
       "Transformers [UNK] fournit des API permettant de t√©l√©charger et d'utiliser rapidement ces mod√®les pr√©-entra√Æn√©s sur un texte donn√©, de les affiner sur vos propres ensembles de donn√©es et de les partager avec la communaut√© sur notre site Web.\n",
       "puis de les partager avec la communaut√© sur notre hub de mod√®les. En m√™me temps, chaque module python d√©finissant une architecture est enti√®rement autonome et peut √™tre modifi√© pour permettre des exp√©riences de recherche rapides.\n",
       "peut √™tre modifi√© pour permettre des exp√©riences de recherche rapides.\n",
       "\n",
       "Pourquoi devrais-je utiliser des transformateurs ?\n",
       "\n",
       "1. Des mod√®les de pointe faciles √† utiliser :\n",
       "  - Haute performance sur les t√¢ches NLU et NLG.\n",
       "  - Faible barri√®re √† l'entr√©e pour les √©ducateurs et les praticiens.\n",
       "  - Peu d'abstractions pour l'utilisateur avec seulement trois classes √† apprendre.\n",
       "  - Une API unifi√©e pour utiliser tous nos mod√®les pr√©-entra√Æn√©s.\n",
       "  - Des co√ªts de calcul plus faibles, une empreinte carbone r√©duite :\n",
       "\n",
       "2. Les chercheurs peuvent partager les mod√®les form√©s au lieu de toujours les reformer.\n",
       "  - Les praticiens peuvent r√©duire le temps de calcul et les co√ªts de production.\n",
       "  - Des dizaines d'architectures avec plus de 10 000 mod√®les pr√©-form√©s, certains dans plus de 100 langues.\n",
       "\n",
       "3. Choisissez le cadre appropri√© pour chaque √©tape de la vie d'un mod√®le :\n",
       "  - Entra√Ænez des mod√®les de pointe en 3 lignes de code.\n",
       "  - D√©placez un seul mod√®le entre les frameworks TF2.0/PyTorch √† volont√©.\n",
       "  - Choisissez de mani√®re transparente le bon framework pour l'entra√Ænement, l'√©valuation et la production.\n",
       "\n",
       "4. Adaptez facilement un mod√®le ou un exemple √† vos besoins :\n",
       "  - Nous fournissons des exemples pour chaque architecture afin de reproduire les r√©sultats publi√©s par ses auteurs originaux.\n",
       "  - Mod√®le interne [SEP]\n",
       "\"\"\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(question, long_context, max_length=384, truncation=\"only_second\")\n",
    "print(tokenizer.decode(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] This sentence is not [SEP]'\n",
       "'[CLS] is not too long [SEP]'\n",
       "'[CLS] too long but we [SEP]'\n",
       "'[CLS] but we are going [SEP]'\n",
       "'[CLS] are going to split [SEP]'\n",
       "'[CLS] to split it anyway [SEP]'\n",
       "'[CLS] it anyway. [SEP]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"This sentence is not too long but we are going to split it anyway.\"\n",
    "# \"Cette phrase n'est pas trop longue mais nous allons la diviser quand m√™me.\"\n",
    "inputs = tokenizer(\n",
    "    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2\n",
    ")\n",
    "\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inputs[\"overflow_to_sample_mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"This sentence is not too long but we are going to split it anyway.\",\n",
    "    # Cette phrase n'est pas trop longue mais nous allons la diviser quand m√™me.\n",
    "    \"This sentence is shorter but will still get split.\",\n",
    "    # Cette phrase est plus courte mais sera quand m√™me divis√©e.\n",
    "]\n",
    "inputs = tokenizer(\n",
    "    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2\n",
    ")\n",
    "\n",
    "print(inputs[\"overflow_to_sample_mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    long_context,\n",
    "    stride=128,\n",
    "    max_length=384,\n",
    "    padding=\"longest\",\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 384)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "offsets = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "inputs = inputs.convert_to_tensors(\"tf\")\n",
    "print(inputs[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 384) (2, 384)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_ids = inputs.sequence_ids()\n",
    "# Masque tout, sauf les tokens du contexte\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "# D√©masquer le jeton [CLS]\n",
    "mask[0] = False\n",
    "# Masquer tous les tokens [PAD]\n",
    "mask = tf.math.logical_or(tf.constant(mask)[None], inputs[\"attention_mask\"] == 0)\n",
    "\n",
    "start_logits = tf.where(mask, -10000, start_logits)\n",
    "end_logits = tf.where(mask, -10000, end_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_probabilities = tf.math.softmax(start_logits, axis=-1).numpy()\n",
    "end_probabilities = tf.math.softmax(end_logits, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 18, 0.33867), (173, 184, 0.97149)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates = []\n",
    "for start_probs, end_probs in zip(start_probabilities, end_probabilities):\n",
    "    scores = start_probs[:, None] * end_probs[None, :]\n",
    "    idx = np.triu(scores).argmax().item()\n",
    "\n",
    "    start_idx = idx // scores.shape[1]\n",
    "    end_idx = idx % scores.shape[1]\n",
    "    score = scores[start_idx, end_idx].item()\n",
    "    candidates.append((start_idx, end_idx, score))\n",
    "\n",
    "print(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '\\nü§ó Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33867}\n",
       "{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.97149}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for candidate, offset in zip(candidates, offsets):\n",
    "    start_token, end_token, score = candidate\n",
    "    start_char, _ = offset[start_token]\n",
    "    _, end_char = offset[end_token]\n",
    "    answer = long_context[start_char:end_char]\n",
    "    result = {\"answer\": answer, \"start\": start_char, \"end\": end_char, \"score\": score}\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Les tokenizers rapides dans le pipeline de QA (TensorFlow)",
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
